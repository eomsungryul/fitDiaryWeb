:tocdepth: 3

Hive Select Statement
====================================

=================
Select Syntax
=================

::

    SELECT [ALL | DISTINCT] select_expr, select_expr, ...
    FROM table_reference
    [WHERE where_condition]
    [GROUP BY col_list]
    [CLUSTER BY col_list
      | [DISTRIBUTE BY col_list] [SORT BY col_list]
    ]
    [LIMIT number]

* A SELECT statement can be part of a :ref:`union <union>` query or a :ref:`subquery <subquery>` of another query.
* table_reference indicates the input to the query. It can be a regular table, a :ref:`view <view>`, a :ref:`join construct <join>` or a :ref:`subquery <subquery>`.
* Simple query. For example, the following query retrieves all columns and all rows from table t1.

------------------------
WHERE Clause
------------------------

The where condition is a boolean [expression]. For example, the following query returns only those sales records which have an amount greater than 10 from the US region. Hive does not support IN, EXISTS or subqueries in the WHERE clause.
::

    SELECT * FROM sales WHERE amount > 10 AND region = "US"

------------------------
ALL and DISTINCT Clauses
------------------------

The ALL and DISTINCT options specify whether duplicate rows should be returned. If none of these options are given, the default is ALL (all matching rows are returned). DISTINCT specifies removal of duplicate rows from the result set.
::

    hive> SELECT col1, col2 FROM t1
        1 3
        1 3
        1 4
        2 5
    hive> SELECT DISTINCT col1, col2 FROM t1
        1 3
        1 4
        2 5
    hive> SELECT DISTINCT col1 FROM t1
        1
        2

------------------------
Partition Based Queries
------------------------

In general, a SELECT query scans the entire table (other than for :ref:`sampling <sampling>`). If a table created using the :ref:`PARTITIONED BY<partition>` clause, a query can do partition pruning and scan only a fraction of the table relevant to the partitions specified by the query. Hive currently does partition pruning if the partition predicates are specified in the WHERE clause or the ON clause in a JOIN. For example, if table page_views is partitioned on column date, the following query retrieves rows for just days between 2008-03-01 and 2008-03-31.
::

    SELECT page_views.*
    FROM page_views
    WHERE page_views.date >= '2008-03-01' AND page_views.date <= '2008-03-31'

If a table page_views is joined with another table dim_users, you can specify a range of partitions in the ON clause as follows:
::

    SELECT page_views.*
    FROM page_views JOIN dim_users
      ON (page_views.user_id = dim_users.id AND page_views.date >= '2008-03-01' AND page_views.date <= '2008-03-31')

* See also :ref:`Group By <group-by>`
* See also :ref:`Sort By <sort-by>` / :ref:`Cluster By <cluster-by>` / :ref:`Distribute By <cluster-by>` / :ref:`Order By <order-by>`

------------------------
HAVING Clause
------------------------

Hive added support for the HAVING clause in version 0.7.0. In older versions of Hive it is possible to achieve the same effect by using a subquery, e.g::

    SELECT col1 FROM t1 GROUP BY col1 HAVING SUM(col2) > 10

can also be expressed as
::

    SELECT col1 FROM (SELECT col1, SUM(col2) AS col2sum FROM t1 GROUP BY col1) t2 WHERE t2.col2sum > 10

------------------------
LIMIT Clause
------------------------

Limit indicates the number of rows to be returned. The rows returned are chosen at random. The following query returns 5 rows from t1 at random.
::

    SELECT * FROM t1 LIMIT 5

Top k queries. The following query returns the top 5 sales records wrt amount.
::

    SET mapred.reduce.tasks = 1
    SELECT * FROM sales SORT BY amount DESC LIMIT 5

---------------------------
REGEX Column Specification
---------------------------

A SELECT statement can take regex-based column specification.

* We use java regex syntax. Try `<http://www.fileformat.info/tool/regex.htm>`_ for testing purposes.
* The following query select all columns except ds and hr.
::

    SELECT `(ds|hr)?+.+` FROM sales


.. _group-by:

=================
Group By
=================

---------------
Group By Syntax
---------------

::

    groupByClause: GROUP BY groupByExpression (, groupByExpression)*

    groupByExpression: expression

    groupByQuery: SELECT expression (, expression)* FROM src groupByClause?

---------------
Simple Examples
---------------

In order to count the number of rows in a table:
::

    SELECT COUNT(*) FROM table2;

.. Note that for versions of Hive which don't include `HIVE-287 <https://issues.apache.org/jira/browse/HIVE-287>`_, you'll need to use COUNT(1) in place of COUNT(*).

In order to count the number of distinct users by gender one could write the following query:
::

    INSERT OVERWRITE TABLE pv_gender_sum
    SELECT pv_users.gender, count (DISTINCT pv_users.userid)
    FROM pv_users
    GROUP BY pv_users.gender;

Multiple aggregations can be done at the same time, however, no two aggregations can have different DISTINCT columns .e.g while the following is possible
::

    INSERT OVERWRITE TABLE pv_gender_agg
    SELECT pv_users.gender, count(DISTINCT pv_users.userid), count(*), sum(DISTINCT pv_users.userid)
    FROM pv_users
    GROUP BY pv_users.gender;

.. Note that for versions of Hive which don't include `HIVE-287 <https://issues.apache.org/jira/browse/HIVE-287>`_, you'll need to use COUNT(1) in place of COUNT(*).

However, the following query is not allowed. We don't allow multiple DISTINCT expressions in the same query.
::

    INSERT OVERWRITE TABLE pv_gender_agg
    SELECT pv_users.gender, count(DISTINCT pv_users.userid), count(DISTINCT pv_users.ip)
    FROM pv_users
    GROUP BY pv_users.gender;

------------------------------------
Select statement and group by clause
------------------------------------

When using group by clause, the select statement can only include columns included in the group by clause. Of course, you can have as many aggregation functions (e.g. count) in the select statement as well.
Let's take a simple example
::

    CREATE TABLE t1(a INTEGER, b INTGER);

A group by query on the above table could look like:
::

    SELECT
       a,
       sum(b)
    FROM
       t1
    GROUP BY
       a;

The above query works because the select clause contains a (the group by key) and an aggregation function (sum(b)).

However, the query below **DOES NOT** work:
::

    SELECT
       a,
       b
    FROM
       t1
    GROUP BY
       a;

This is because the select clause has an additional column (b) that is not included in the group by clause (and it's not an aggregation function either). This is because, if the table t1 looked like:
::

    a    b
    ------
    100  1
    100  2
    100  3

Since the grouping is only done on a, what value of b should Hive display for the group a=100? One can argue that it should be the first value or the lowest value but we all agree that there are multiple possible options. Hive does away with this guessing by making it invalid SQL (HQL, to be precise) to have a column in the select clause that is not included in the group by clause.

--------------------
Advanced Features
--------------------

^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Multi-Group-By Inserts
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The output of the aggregations or simple selects can be further sent into multiple tables or even to hadoop dfs files (which can then be manipulated using hdfs utilitites). e.g. if along with the gender breakdown, one needed to find the breakdown of unique page views by age, one could accomplish that with the following query:
::

  FROM pv_users 
  INSERT OVERWRITE TABLE pv_gender_sum
    SELECT pv_users.gender, count(DISTINCT pv_users.userid) 
    GROUP BY pv_users.gender 
  INSERT OVERWRITE DIRECTORY '/user/facebook/tmp/pv_age_sum'
    SELECT pv_users.age, count(DISTINCT pv_users.userid) 
    GROUP BY pv_users.age; 

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Map-side Aggregation for Group By
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

hive.map.aggr controls how we do aggregations. The default is false. If it is set to true, Hive will do the first-level aggregation directly in the map task.
This usually provides better efficiency, but may require more memory to run successfully.
::

  set hive.map.aggr=true;
  SELECT COUNT(*) FROM table2;

.. Note that for versions of Hive which don't include HIVE-287, you'll need to use COUNT(1) in place of COUNT.

.. _order-by:

===============================================
Order By
===============================================

The ORDER BY syntax in Hive QL is similar to the syntax of ORDER BY in SQL language.

::

    colOrder: ( ASC | DESC )
    orderBy: ORDER BY colName colOrder? (',' colName colOrder?)*
    query: SELECT expression (',' expression)* FROM src orderBy

There are some limitations in the "order by" clause. In the strict mode (i.e., hive.mapred.mode=strict), the order by clause has to be followed by a "limit" clause. The limit clause is not necessary if you set hive.mapred.mode to nonstrict. The reason is that in order to impose total order of all results, there has to be one reducer to sort the final output. If the number of rows in the output is too large, the single reducer could take a very long time to finish.

.. _sort-by:

===============================================
Sort By
===============================================

The SORT BY syntax is similar to the syntax of ORDER BY in SQL language.

::

    colOrder: ( ASC | DESC )
    sortBy: SORT BY colName colOrder? (',' colName colOrder?)*
    query: SELECT expression (',' expression)* FROM src sortBy

Hive uses the columns in SORT BY to sort the rows before feeding the rows to a reducer. The sort order will be dependent on the column types. If the column is of numeric type, then the sort order is also in numeric order. If the column is of string type, then the sort order will be lexicographical order.

-------------------------------------------
Difference between Sort By and Order By
-------------------------------------------

Hive supports SORT BY which sorts the data per reducer. The difference between "order by" and "sort by" is that the former guarantees total order in the output while the latter only guarantees ordering of the rows within a reducer. If there are more than one reducer, "sort by" may give partially ordered final results.

Note: It may be confusing as to the difference between SORT BY alone of a single column and CLUSTER BY. The difference is that CLUSTER BY partitions by the field and SORT BY if there are multiple reducers partitions randomly in order to distribute data (and load) uniformly across the reducers.

Basically, the data in each reducer will be sorted according to the order that the user specified. The following example shows

::

    SELECT key, value FROM src SORT BY key ASC, value DESC

The query had 2 reducers, and the output of each is::

    0   5
    0   3
    3   6
    9   1

::

    0   4
    0   3
    1   1
    2   5

------------------------------
Setting Types for Sort By
------------------------------

After a transform, variable types are generally considered to be strings, meaning that numeric data will be sorted lexicographically. To overcome this, a second SELECT statement with casts can be used before using SORT BY.

::

    FROM (FROM (FROM src
                SELECT TRANSFORM(value)
                USING 'mapper'
                AS value, count) mapped
          SELECT cast(value as double) AS value, cast(count as int) AS count
          SORT BY value, count) sorted
    SELECT TRANSFORM(value, count)
    USING 'reducer'
    AS whatever

.. _cluster-by:

===============================================
Distribute By and Cluster By
===============================================

Cluster By and Distribute By are used mainly with the `Transform/Map-Reduce Scripts <https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Transform>`_ . But, it is sometimes useful in SELECT statements if there is a need to partition and sort the output of a query for subsequent queries.

Cluster By is a short-cut for both Distribute By and Sort By.

Hive uses the columns in Distribute By to distribute the rows among reducers. All rows with the same Distribute By columns will go to the same reducer. However, Distribute By does not guarantee clustering or sorting properties on the distributed keys.

For example, we are Distributing By x on the following 5 rows to 2 reducer::

    x1
    x2
    x4
    x3
    x1

Reducer 1 got
::

    x1
    x2
    x1

Reducer 2 got
::

    x4
    x3

Note that all rows with the same key x1 is guaranteed to be distributed to the same reducer (reducer 1 in this case), but they are not guaranteed to be clustered in adjacent positions.

In contrast, if we use Cluster By x, the two reducers will further sort rows on x:

Reducer 1 got
::

    x1
    x1
    x2

Reducer 2 got
::

    x3
    x4

Instead of specifying Cluster By, the user can specify Distribute By and Sort By, so the partition columns and sort columns can be different. The usual case is that the partition columns are a prefix of sort columns, but that is not required.
::

    SELECT col1, col2 FROM t1 CLUSTER BY col1

::

    SELECT col1, col2 FROM t1 DISTRIBUTE BY col1

    SELECT col1, col2 FROM t1 DISTRIBUTE BY col1 SORT BY col1 ASC, col2 DESC

::

      FROM (
        FROM pv_users
        MAP ( pv_users.userid, pv_users.date )
        USING 'map_script'
        AS c1, c2, c3
        DISTRIBUTE BY c2
        SORT BY c2, c1) map_output
      INSERT OVERWRITE TABLE pv_users_reduced
        REDUCE ( map_output.c1, map_output.c2, map_output.c3 )
        USING 'reduce_script'
        AS date, count;

..

    =========================================
    Transform and Map-Reduce Scripts
    =========================================

.. _join:

=================
Joins
=================

-----------------
Join Syntax
-----------------

Hive supports the following syntax for joining tables::

    join_table:
        table_reference JOIN table_factor [join_condition]
      | table_reference {LEFT|RIGHT|FULL} [OUTER] JOIN table_reference join_condition
      | table_reference LEFT SEMI JOIN table_reference join_condition
      | table_reference CROSS JOIN table_reference [join_condition] (as of Hive 0.10)

    table_reference:
        table_factor
      | join_table

    table_factor:
        tbl_name [alias]
      | table_subquery alias
      | ( table_references )

    join_condition:
        ON equality_expression ( AND equality_expression )*

    equality_expression:
        expression = expression

Only equality joins, outer joins, and left semi joins are supported in Hive. Hive does not support join conditions that are not equality conditions as it is very difficult to express such conditions as a map/reduce job. Also, more than two tables can be joined in Hive.

--------------
Examples
--------------

Some salient points to consider when writing join queries are as follows:

Only equality joins are allowed e.g.

::

    SELECT a.* FROM a JOIN b ON (a.id = b.id)

::

    SELECT a.* FROM a JOIN b ON (a.id = b.id AND a.department = b.department)

are both valid joins, however

::

    SELECT a.* FROM a JOIN b ON (a.id <> b.id)

is NOT allowed

More than 2 tables can be joined in the same query e.g.

::

    SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2)

is a valid join.

Hive converts joins over multiple tables into a single map/reduce job if for every table the same column is used in the join clauses e.g.

::

    SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1)

is converted into a single map/reduce job as only key1 column for b is involved in the join. On the other hand

::

    SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2)

is converted into two map/reduce jobs because key1 column from b is used in the first join condition and key2 column from b is used in the second one. The first map/reduce job joins a with b and the results are then joined with c in the second map/reduce job.

In every map/reduce stage of the join, the last table in the sequence is streamed through the reducers where as the others are buffered. Therefore, it helps to reduce the memory needed in the reducer for buffering the rows for a particular value of the join key by organizing the tables such that the largest tables appear last in the sequence. e.g. in

::

    SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1)

all the three tables are joined in a single map/reduce job and the values for a particular value of the key for tables a and b are buffered in the memory in the reducers. Then for each row retrieved from c, the join is computed with the buffered rows. Similarly for

::

    SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2)

there are two map/reduce jobs involved in computing the join. The first of these joins a with b and buffers the values of a while streaming the values of b in the reducers. The second of one of these jobs buffers the results of the first join while streaming the values of c through the reducers.

In every map/reduce stage of the join, the table to be streamed can be specified via a hint. e.g. in

::

    SELECT /*+ STREAMTABLE(a) */ a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1)

all the three tables are joined in a single map/reduce job and the values for a particular value of the key for tables b and c are buffered in the memory in the reducers. Then for each row retrieved from a, the join is computed with the buffered rows.

LEFT, RIGHT, and FULL OUTER joins exist in order to provide more control over ON clauses for which there is no match. For example, this query:

::

    SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key)

will return a row for every row in a. This output row will be a.val,b.val when there is a b.key that equals a.key, and the output row will be a.val,NULL when there is no corresponding b.key. Rows from b which have no corresponding a.key will be dropped. The syntax "FROM a LEFT OUTER JOIN b" must be written on one line in order to understand how it works--a is to the LEFT of b in this query, and so all rows from a are kept; a RIGHT OUTER JOIN will keep all rows from b, and a FULL OUTER JOIN will keep all rows from a and all rows from b. OUTER JOIN semantics should conform to standard SQL specs.

Joins occur BEFORE WHERE CLAUSES. So, if you want to restrict the OUTPUT of a join, a requirement should be in the WHERE clause, otherwise it should be in the JOIN clause. A big point of confusion for this issue is partitioned tables:

::

    SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key)
    WHERE a.ds='2009-07-07' AND b.ds='2009-07-07'

will join a on b, producing a list of a.val and b.val. The WHERE clause, however, can also reference other columns of a and b that are in the output of the join, and then filter them out. However, whenever a row from the JOIN has found a key for a and no key for b, all of the columns of b will be NULL, including the ds column. This is to say, you will filter out all rows of join output for which there was no valid b.key, and thus you have outsmarted your LEFT OUTER requirement. In other words, the LEFT OUTER part of the join is irrelevant if you reference any column of b in the WHERE clause. Instead, when OUTER JOINing, use this syntax:

::

    SELECT a.val, b.val FROM a LEFT OUTER JOIN b
    ON (a.key=b.key AND b.ds='2009-07-07' AND a.ds='2009-07-07')

..the result is that the output of the join is pre-filtered, and you won't get post-filtering trouble for rows that have a valid a.key but no matching b.key. The same logic applies to RIGHT and FULL joins.

Joins are NOT commutative! Joins are left-associative regardless of whether they are LEFT or RIGHT joins.

::

    SELECT a.val1, a.val2, b.val, c.val
    FROM a
    JOIN b ON (a.key = b.key)
    LEFT OUTER JOIN c ON (a.key = c.key)

...first joins a on b, throwing away everything in a or b that does not have a corresponding key in the other table. The reduced table is then joined on c. This provides unintuitive results if there is a key that exists in both a and c but not b: The whole row (including a.val1, a.val2, and a.key) is dropped in the "a JOIN b" step because it is not in b. The result does not have a.key in it, so when it is LEFT OUTER JOINed with c, c.val does not make it in because there is no c.key that matches an a.key (because that row from a was removed). Similarly, if this were a RIGHT OUTER JOIN (instead of LEFT), we would end up with an even weirder effect: NULL, NULL, NULL, c.val, because even though we specified a.key=c.key as the join key, we dropped all rows of a that did not match the first JOIN.
To achieve the more intuitive effect, we should instead do FROM c LEFT OUTER JOIN a ON (c.key = a.key) LEFT OUTER JOIN b ON (c.key = b.key).

LEFT SEMI JOIN implements the correlated IN/EXISTS subquery semantics in an efficient way. Since Hive currently does not support IN/EXISTS subqueries, you can rewrite your queries using LEFT SEMI JOIN. The restrictions of using LEFT SEMI JOIN is that the right-hand-side table should only be referenced in the join condition (ON-clause), but not in WHERE- or SELECT-clauses etc.

::

    SELECT a.key, a.value
    FROM a
    WHERE a.key in
     (SELECT b.key
      FROM B);

can be rewritten to:

::

    SELECT a.key, a.val
    FROM a LEFT SEMI JOIN b on (a.key = b.key)

If all but one of the tables being joined are small, the join can be performed as a map only job. The query

::
    
    SELECT /*+ MAPJOIN(b) */ a.key, a.value
    FROM a join b on a.key = b.key

does not need a reducer. For every mapper of A, B is read completely. The restriction is that a FULL/RIGHT OUTER JOIN b cannot be performed

If the tables being joined are bucketized on the join columns, and the number of buckets in one table is a multiple of the number of buckets in the other table, the buckets can be joined with each other. If table A has 4 buckets and table B has 4 buckets, the following join

::

    SELECT /*+ MAPJOIN(b) */ a.key, a.value
    FROM a join b on a.key = b.key

can be done on the mapper only. Instead of fetching B completely for each mapper of A, only the required buckets are fetched. For the query above, the mapper processing bucket 1 for A will only fetch bucket 1 of B. It is not the default behavior, and is governed by the following parameter

::

  set hive.optimize.bucketmapjoin = true


If the tables being joined are sorted and bucketized on the join columns, and they have the same number of buckets, a sort-merge join can be performed. The corresponding buckets are joined with each other at the mapper. If both And B have 4 buckets,

::

    SELECT /*+ MAPJOIN(b) */ a.key, a.value
    FROM A a join B b on a.key = b.key

can be done on the mapper only. The mapper for the bucket for A will traverse the corresponding bucket for B. This is not the default behavior, and the following parameters need to be set:

::

    set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
    set hive.optimize.bucketmapjoin = true;
    set hive.optimize.bucketmapjoin.sortedmerge = true;

.. _union:

=================
Union
=================

::

    select_statement UNION ALL select_statement UNION ALL select_statement ...

UNION is used to combine the result from multiple SELECT statements into a single result set. We currently only support UNION ALL (bag union) i.e. duplicates are not eliminated. The number and names of columns returned by each select_statement has to be the same. Otherwise, a schema error is thrown.

If some additional processing has to be done on the result of the UNION, the entire statement expression can be embedded in a FROM clause like below:
::

    SELECT *
    FROM (
      select_statement
      UNION ALL
      select_statement
    ) unionResult

For example, if we suppose there are two different tables that track which user has published a video and which user has published a comment, the following query joins the results of a union all with the user table to create a single annotated stream for all the video publishing and comment publishing events:
::

    SELECT u.id, actions.date
    FROM (
        SELECT av.uid AS uid 
        FROM action_video av 
        WHERE av.date = '2008-06-03' 
        UNION ALL 
        SELECT ac.uid AS uid 
        FROM action_comment ac 
        WHERE ac.date = '2008-06-03' 
     ) actions JOIN users u ON (u.id = actions.uid) 

.. _subquery:

========================
SubQueries
========================

::

    SELECT ... FROM (subquery) name ...

Hive supports subqueries only in the FROM clause. The subquery has to be given a name because every table in a FROM clause must have a name. Columns in the subquery select list must have unique names. The columns in the subquery select list are available in the outer query just like columns of a table. The subquery can also be a query expression with UNION. Hive supports arbitrary levels of sub-queries.

Example with simple subquery::

    SELECT col 
    FROM (
      SELECT a+b AS col
      FROM t1
    ) t2

Example with subquery containing a UNION ALL::

    SELECT t3.col
    FROM (
      SELECT a+b AS col
      FROM t1
      UNION ALL
      SELECT c+d AS col
      FROM t2
    ) t3


.. _sampling:

======================
Sampling
======================

--------------------------
Sampling Bucketized Table
--------------------------

::

    table_sample: TABLESAMPLE (BUCKET x OUT OF y [ON colname])

The TABLESAMPLE clause allows the users to write queries for samples of the data instead of the whole table. The TABLESAMPLE clause can be added to any table in the FROM clause. The buckets are numbered starting from 1. colname indicates the column on which to sample each row in the table. colname can be one of the non-partition columns in the table or rand() indicating sampling on the entire row instead of an individual column. The rows of the table are 'bucketed' on the colname randomly into y buckets numbered 1 through y. Rows which belong to bucket x are returned.

In the following example the 3rd bucket out of the 32 buckets of the table source. 's' is the table alias.
::

    SELECT *
    FROM source TABLESAMPLE(BUCKET 3 OUT OF 32 ON rand()) s;

Input pruning: Typically, TABLESAMPLE will scan the entire table and fetch the sample. But, that is not very efficient. Instead, the table can be created with a CLUSTERED BY clause which indicates the set of columns on which the table is hash-partitioned/clustered on. If the columns specified in the TABLESAMPLE clause match the columns in the CLUSTERED BY clause, TABLESAMPLE scans only the required hash-partitions of the table.

Example:

So in the above example, if table 'source' was created with 'CLUSTERED BY id INTO 32 BUCKETS'
::

    TABLESAMPLE(BUCKET 3 OUT OF 16 ON id)

would pick out the 3rd and 19th clusters as each bucket would be composed of (32/16)=2 clusters.

On the other hand the tablesample clause
::

    TABLESAMPLE(BUCKET 3 OUT OF 64 ON id)

would pick out half of the 3rd cluster as each bucket would be composed of (32/64)=1/2 of a cluster.

---------------------
Block Sampling
---------------------

Block sampling is available starting with Hive 0.8. Addressed under JIRA - `HIVE-2121 <https://issues.apache.org/jira/browse/HIVE-2121>`_
::

    block_sample: TABLESAMPLE (n PERCENT)

This will allow Hive to pick up at least n% data size (notice it doesn't necessarily mean number of rows) as inputs. Only CombineHiveInputFormat is supported and some special compression formats are not handled. If we fail to sample it, the input of MapReduce job will be the whole table/partition. We do it in HDFS block level so that the sampling granularity is block size. For example, if block size is 256MB, even if n% of input size is only 100MB, you get 256MB of data.

In the following example the input size 0.1% or more will be used for the query.
::

    SELECT *
    FROM source TABLESAMPLE(0.1 PERCENT) s;

Sometimes you want to sample the same data with different blocks, you can change this seed number:
::

    set hive.sample.seednumber=<INTEGER>;

Or user can specify total length to be read, but it has same limitation with PERCENT sampling. (As of Hive 0.10.0 - `HIVE-3401 <https://issues.apache.org/jira/browse/HIVE-3401>`_)
::

    block_sample: TABLESAMPLE (ByteLengthLiteral)

    ByteLengthLiteral : (Digit)+ ('b' | 'B' | 'k' | 'K' | 'm' | 'M' | 'g' | 'G')

In the following example the input size 100M or more will be used for the query.
::

    SELECT *
    FROM source TABLESAMPLE(100M) s;

Hive also supports limiting input by row count basis, but it acts differently with above two. First, it does not need CombineHiveInputFormat which means this can be used with non-native tables. Second, the row count given by user is applied to each split. So total row count can be vary by number of input splits. (As of Hive 0.10.0 - `HIVE-3401 <https://issues.apache.org/jira/browse/HIVE-3401>`_)
::

    block_sample: TABLESAMPLE (n ROWS)

For example, the following query will take the first 10 rows from each input split.
::

    SELECT * FROM source TABLESAMPLE(10 ROWS);

